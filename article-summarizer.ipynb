{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Mining and Applied NLP (44-620)\n",
    "\n",
    "## Final Project: Article Summarizer\n",
    "\n",
    "### Student Name: Brandon Shellenberger\n",
    "#### GitHub Link: https://github.com/Bshell13/shellenberger_module_07\n",
    "\n",
    "Perform the tasks described in the Markdown cells below.  When you have completed the assignment make sure your code cells have all been run (and have output beneath them) and ensure you have committed and pushed ALL of your changes to your assignment repository.\n",
    "\n",
    "You should bring in code from previous assignments to help you answer the questions below.\n",
    "\n",
    "Every question that requires you to write code will have a code cell underneath it; you may either write your entire solution in that cell or write it in a python file (`.py`), then import and run the appropriate code to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages Installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible Prereqs installed:\n",
      "Package            Version"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ ------------\n",
      "annotated-types    0.7.0\n",
      "asttokens          2.4.1\n",
      "beautifulsoup4     4.12.3\n",
      "blis               1.0.1\n",
      "catalogue          2.0.10\n",
      "certifi            2024.8.30\n",
      "charset-normalizer 3.4.0\n",
      "click              8.1.7\n",
      "cloudpathlib       0.20.0\n",
      "colorama           0.4.6\n",
      "comm               0.2.2\n",
      "confection         0.1.5\n",
      "contourpy          1.3.1\n",
      "cycler             0.12.1\n",
      "cymem              2.0.10\n",
      "debugpy            1.8.9\n",
      "decorator          5.1.1\n",
      "executing          2.1.0\n",
      "fonttools          4.55.0\n",
      "html5lib           1.1\n",
      "idna               3.10\n",
      "ipykernel          6.29.5\n",
      "ipython            8.30.0\n",
      "jedi               0.19.2\n",
      "Jinja2             3.1.4\n",
      "joblib             1.4.2\n",
      "jupyter_client     8.6.3\n",
      "jupyter_core       5.7.2\n",
      "kiwisolver         1.4.7\n",
      "langcodes          3.5.0\n",
      "language_data      1.3.0\n",
      "marisa-trie        1.2.1\n",
      "markdown-it-py     3.0.0\n",
      "MarkupSafe         3.0.2\n",
      "matplotlib         3.9.2\n",
      "matplotlib-inline  0.1.7\n",
      "mdurl              0.1.2\n",
      "murmurhash         1.0.11\n",
      "nest-asyncio       1.6.0\n",
      "nltk               3.9.1\n",
      "numpy              2.0.2\n",
      "packaging          24.2\n",
      "parso              0.8.4\n",
      "pillow             11.0.0\n",
      "pip                24.0\n",
      "platformdirs       4.3.6\n",
      "preshed            3.0.9\n",
      "prompt_toolkit     3.0.48\n",
      "psutil             6.1.0\n",
      "pure_eval          0.2.3\n",
      "pydantic           2.10.2\n",
      "pydantic_core      2.27.1\n",
      "Pygments           2.18.0\n",
      "pyparsing          3.2.0\n",
      "python-dateutil    2.9.0.post0\n",
      "pywin32            308\n",
      "pyzmq              26.2.0\n",
      "regex              2024.11.6\n",
      "requests           2.32.3\n",
      "rich               13.9.4\n",
      "setuptools         75.6.0\n",
      "shellingham        1.5.4\n",
      "six                1.16.0\n",
      "smart-open         7.0.5\n",
      "soupsieve          2.6\n",
      "spacy              3.8.2\n",
      "spacy-legacy       3.0.12\n",
      "spacy-loggers      1.0.5\n",
      "spacytextblob      5.0.0\n",
      "srsly              2.4.8\n",
      "stack-data         0.6.3\n",
      "textblob           0.18.0.post0\n",
      "thinc              8.3.2\n",
      "tornado            6.4.2\n",
      "tqdm               4.67.1\n",
      "traitlets          5.14.3\n",
      "typer              0.14.0\n",
      "typing_extensions  4.12.2\n",
      "urllib3            2.2.3\n",
      "wasabi             1.1.3\n",
      "wcwidth            0.2.13\n",
      "weasel             0.4.1\n",
      "webencodings       0.5.1\n",
      "wrapt              1.17.0\n"
     ]
    }
   ],
   "source": [
    "print('Possible Prereqs installed:')\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 - Extract HTML from website\n",
    "Find on the internet an article or blog post about a topic that interests you and you are able to get the text for using the technologies we have applied in the course.  Get the html for the article and store it in a file (which you must submit with your project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "text/html; charset=utf-8\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pickle\n",
    "response = requests.get('https://www.mlb.com/news/royals-2014-season-film')\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.headers['content-type'])\n",
    "\n",
    "with open('shellenberger_module_07.pkl', 'wb') as file:\n",
    "    pickle.dump(response.text, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 - Sentiment and Number of Sentences\n",
    "Read in your article's html source from the file you created in question 1 and do sentiment analysis on the article/post's text (use `.get_text()`).  Print the polarity score with an appropriate label.  Additionally print the number of sentences in the original article (with an appropriate label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The polarity score of this article is 0.19.\n",
      "The number of sentences in this article is 35.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "def extract_text(file_name:str):\n",
    "    '''\n",
    "    Extracts text from an HTML file using BeautifulSoup\n",
    "    The file needs to be stored in the same directory as this file\n",
    "    '''\n",
    "    with open(file_name, 'rb') as file:\n",
    "        article_html = pickle.load(file)\n",
    "\n",
    "    parser = 'html.parser'\n",
    "    soup = BeautifulSoup(article_html, parser)\n",
    "    article_element = soup.find('article')\n",
    "    return article_element.get_text()\n",
    "\n",
    "def extract_doc(article_text):\n",
    "    '''\n",
    "    This is used to fine the polarity score of text\n",
    "    Download \"en_core_web_sm\" before working\n",
    "    Type into terminal \"py -m spacy download en_core_web_sm\"\n",
    "    '''\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    nlp.add_pipe('spacytextblob')\n",
    "    return nlp(article_text)\n",
    "\n",
    "def num_of_sentences(doc):\n",
    "    '''\n",
    "    Determines the nubmer of sentences in a text\n",
    "    '''\n",
    "    sentences = list(doc.sents)\n",
    "    return len(sentences)    \n",
    "\n",
    "file_name = 'shellenberger_module_07.pkl'\n",
    "article_text = extract_text(file_name)\n",
    "doc = extract_doc(article_text)\n",
    "print(f'The polarity score of this article is {doc._.blob.polarity:.2f}.')\n",
    "print(f'The number of sentences in this article is {num_of_sentences(doc)}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 - Most Frequent Tokens\n",
    "Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent tokens (converted to lower case).  Print the common tokens with an appropriate label.  Additionally, print the tokens their frequencies (with appropriate labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tope 5 Most Frequent Tokens:\n",
      "The token royals shows up 11 times in the article.\n",
      "The token hosmer shows up 10 times in the article.\n",
      "The token like shows up 9 times in the article.\n",
      "The token run shows up 8 times in the article.\n",
      "The token team shows up 8 times in the article.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def most_frequent_tokens(doc):\n",
    "    '''\n",
    "    Counts the number of words and their variations and sets them into a list\n",
    "    The returns the top 5 most used words in the text\n",
    "    Stores values as such [(word_1, frequency), (word_2, frequency), ...]'''\n",
    "    interesting_tokens = [token.text.lower() for token in doc if token.is_alpha and not(token.is_space or token.is_punct or token.is_stop)]\n",
    "    return Counter(interesting_tokens).most_common(5)\n",
    "\n",
    "most_common_tokens = most_frequent_tokens(doc)\n",
    "\n",
    "print('Tope 5 Most Frequent Tokens:')\n",
    "# Extracts individual elements into word and frequency\n",
    "# and prints each word and frequency respectively.\n",
    "for word, freq in most_common_tokens:\n",
    "    print(f'The token {word} shows up {freq} times in the article.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 - Most Frequent Lemmas\n",
    "Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent lemmas (converted to lower case).  Print the common lemmas with an appropriate label.  Additionally, print the lemmas with their frequencies (with appropriate labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tope 5 Most Frequent Lemmas:\n",
      "The lemma token hosmer shows up 10 times in the article.\n",
      "The lemma token like shows up 9 times in the article.\n",
      "The lemma token run shows up 8 times in the article.\n",
      "The lemma token team shows up 8 times in the article.\n",
      "The lemma token time shows up 7 times in the article.\n"
     ]
    }
   ],
   "source": [
    "def most_frequent_lemmas(doc):\n",
    "    '''\n",
    "    Counts the number of lemma words and their variations and sets them into a list\n",
    "    The returns the top 5 most used words in the text\n",
    "    Stores values as such [(word_1, frequency), (word_2, frequency), ...]'''\n",
    "    interesting_tokens_lemma = [token.lemma_.lower() for token in doc if token.is_alpha and not(token.is_space or token.is_punct or token.is_stop)]\n",
    "    return Counter(interesting_tokens_lemma).most_common(5)\n",
    "\n",
    "most_common_lemmas = most_frequent_lemmas(doc)\n",
    "\n",
    "print('Tope 5 Most Frequent Lemmas:')\n",
    "# Extracts individual elements into word and frequency\n",
    "# and prints each word and frequency respectively.\n",
    "for word_lemma, freq_lemma in most_common_lemmas:\n",
    "    print(f'The lemma token {word_lemma} shows up {freq_lemma} times in the article.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Make a list containing the scores (using tokens) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores. From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Make a list containing the scores (using lemmas) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores.  From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Using the histograms from questions 5 and 6, decide a \"cutoff\" score for tokens and lemmas such that fewer than half the sentences would have a score greater than the cutoff score.  Record the scores in this Markdown cell\n",
    "\n",
    "* Cutoff Score (tokens): \n",
    "* Cutoff Score (lemmas):\n",
    "\n",
    "Feel free to change these scores as you generate your summaries.  Ideally, we're shooting for at least 6 sentences for our summary, but don't want more than 10 (these numbers are rough estimates; they depend on the length of your article)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on tokens) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Print the polarity score of your summary you generated with the token scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on lemmas) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Print the polarity score of your summary you generated with the lemma scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.  Compare your polarity scores of your summaries to the polarity scores of the initial article.  Is there a difference?  Why do you think that may or may not be?.  Answer in this Markdown cell.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Based on your reading of the original article, which summary do you think is better (if there's a difference).  Why do you think this might be?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
